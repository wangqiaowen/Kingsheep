{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'OLAT_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-80d945112d69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkingsheep_training\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKingsheepEnv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrl_player_training\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRLPlayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msimple_player\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSimplePlayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\source\\pai-2019\\assignment_3\\OLAT_code\\training\\kingsheep_training.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mOLAT_code\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mKingsheepEnv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'OLAT_code'"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "import random\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from kingsheep_training import KingsheepEnv\n",
    "from rl_player_training import RLPlayer\n",
    "from simple_player import SimplePlayer\n",
    "from config import FIELD_WIDTH, FIELD_HEIGHT\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_inputs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=n_inputs, out_features=24)\n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=5)\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = t.flatten(start_dim=1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t\n",
    "\n",
    "\n",
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "\n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size\n",
    "\n",
    "\n",
    "class EpsilonGreedyStrategy:\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * current_step * self.decay)\n",
    "\n",
    "\n",
    "class KingsheepEnvManager:\n",
    "    def __init__(self, device, player1, player2, map_name):\n",
    "        self.device = device\n",
    "        self.player1 = player1\n",
    "        self.player2 = player2\n",
    "        self.map_name = map_name\n",
    "        self.env = KingsheepEnv(player1=player1, player2=player2, map_name=map_name)\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.done = False\n",
    "        self.env = KingsheepEnv(player1=self.player1, player2=self.player2, map_name=self.map_name)\n",
    "\n",
    "    def step(self):\n",
    "        iteration_summary, self.done = self.env.step()\n",
    "        for k in iteration_summary.keys():\n",
    "            for a in iteration_summary[k].keys():\n",
    "                if a in {\"state\", \"next_state\"}:\n",
    "                    iteration_summary[k][a] = RLPlayer.convert_field_to_state(field=iteration_summary[k][a],\n",
    "                                                                              device=self.device)\n",
    "                else:\n",
    "                    iteration_summary[k][a] = torch.tensor([iteration_summary[k][a]], device=self.device)\n",
    "\n",
    "        return iteration_summary\n",
    "\n",
    "\n",
    "def plot(values, moving_avg_period):\n",
    "    if is_ipython: display.clear_output(wait=True)\n",
    "\n",
    "    plt.figure(2)\n",
    "\n",
    "    plt.plot(values)\n",
    "\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)\n",
    "\n",
    "    plt.title(\"Training...\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Episode\", len(values), \"\\n\", moving_avg_period, \"episode moving avg:\", moving_avg[-1])\n",
    "\n",
    "\n",
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1).mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period - 1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()\n",
    "\n",
    "\n",
    "def extract_tensors(experiences):\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "\n",
    "    return t1, t2, t3, t4\n",
    "\n",
    "\n",
    "class QValues:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_next(target_net, next_states):\n",
    "        final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        batch_size = next_states.shape[0]\n",
    "\n",
    "        values = torch.zeros(batch_size).to(QValues.device)\n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        return values\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 10\n",
    "memory_size = 100000\n",
    "lr = 0.001\n",
    "num_episodes = 500\n",
    "\n",
    "\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "input_size = FIELD_WIDTH * FIELD_HEIGHT\n",
    "\n",
    "sheep_policy_net = DQN(input_size).to(device)\n",
    "sheep_optimizer = optim.Adam(params=sheep_policy_net.parameters(), lr=lr)\n",
    "\n",
    "wolf_policy_net = DQN(input_size).to(device)\n",
    "wolf_optimizer = optim.Adam(params=wolf_policy_net.parameters(), lr=lr)\n",
    "\n",
    "sheep_target_net = DQN(input_size).to(device)\n",
    "sheep_target_net.load_state_dict(sheep_policy_net.state_dict())\n",
    "sheep_target_net.eval()\n",
    "\n",
    "wolf_target_net = DQN(input_size).to(device)\n",
    "wolf_target_net.load_state_dict(wolf_policy_net.state_dict())\n",
    "wolf_target_net.eval()\n",
    "\n",
    "sheep_memory = ReplayMemory(memory_size)\n",
    "wolf_memory = ReplayMemory(memory_size)\n",
    "\n",
    "player1 = RLPlayer(sheep_policy_net=sheep_policy_net,\n",
    "                   wolf_policy_net=wolf_policy_net,\n",
    "                   strategy=strategy,\n",
    "                   device=device)\n",
    "player2 = SimplePlayer()\n",
    "\n",
    "is_p1_rl = True\n",
    "\n",
    "map_name = r'.\\resources\\test.map'\n",
    "\n",
    "em = KingsheepEnvManager(device=device, player1=player1, player2=player2, map_name=map_name)\n",
    "\n",
    "episode_rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    em.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    while True:\n",
    "        iteration_summary = em.step()\n",
    "\n",
    "        if is_p1_rl:\n",
    "            if 'sheep1' in iteration_summary:\n",
    "                sheep_memory.push(Experience(iteration_summary['sheep1']['state'],\n",
    "                                             iteration_summary['sheep1']['move'],\n",
    "                                             iteration_summary['sheep1']['next_state'],\n",
    "                                             iteration_summary['sheep1']['reward']))\n",
    "                episode_reward += iteration_summary['sheep1']['reward']\n",
    "\n",
    "            if 'wolf1' in iteration_summary:\n",
    "                wolf_memory.push(Experience(iteration_summary['wolf1']['state'],\n",
    "                                            iteration_summary['wolf1']['move'],\n",
    "                                            iteration_summary['wolf1']['next_state'],\n",
    "                                            iteration_summary['wolf1']['reward']))\n",
    "                episode_reward += iteration_summary['wolf1']['reward']\n",
    "\n",
    "        else:\n",
    "            if 'sheep2' in iteration_summary:\n",
    "                sheep_memory.push(Experience(iteration_summary['sheep2']['state'],\n",
    "                                             iteration_summary['sheep2']['move'],\n",
    "                                             iteration_summary['sheep2']['next_state'],\n",
    "                                             iteration_summary['sheep2']['reward']))\n",
    "                episode_reward += iteration_summary['sheep2']['reward']\n",
    "\n",
    "            if 'wolf2' in iteration_summary:\n",
    "                wolf_memory.push(Experience(iteration_summary['wolf2']['state'],\n",
    "                                            iteration_summary['wolf2']['move'],\n",
    "                                            iteration_summary['wolf2']['next_state'],\n",
    "                                            iteration_summary['wolf2']['reward']))\n",
    "                episode_reward += iteration_summary['wolf2']['reward']\n",
    "\n",
    "        for memory, policy_net, target_net, optimizer in zip([sheep_memory, wolf_memory],\n",
    "                                                             [sheep_policy_net, wolf_policy_net],\n",
    "                                                             [sheep_target_net, wolf_target_net],\n",
    "                                                             [sheep_optimizer, wolf_optimizer]):\n",
    "            if memory.can_provide_sample(batch_size):\n",
    "                experiences = memory.sample(batch_size)\n",
    "                states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "\n",
    "                current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "                next_q_values = QValues.get_next(target_net, next_states)\n",
    "                target_q_values = (next_q_values * gamma) + rewards\n",
    "\n",
    "                loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        if em.done:\n",
    "            episode_rewards.append(episode_reward)\n",
    "            plot(episode_rewards, 25)\n",
    "            break\n",
    "\n",
    "    if episode % target_update == 0:\n",
    "        for policy_net, target_net in zip([sheep_policy_net, wolf_policy_net],\n",
    "                                          [sheep_target_net, wolf_target_net]):\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "torch.save(sheep_policy_net.state_dict(), 'rlplayer_sheep_model.pt')\n",
    "torch.save(wolf_policy_net.state_dict(), 'rlplayer_wolf_model.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
